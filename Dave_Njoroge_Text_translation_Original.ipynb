{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dave_Njoroge_Text translation-Original",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaveChui/Neural-Networks-IP/blob/main/Dave_Njoroge_Text_translation_Original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Translation Using Neural Networks"
      ],
      "metadata": {
        "id": "lVIogMSUpCEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Defining the Question"
      ],
      "metadata": {
        "id": "3ZF_Do3ln3nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Link to documentation"
      ],
      "metadata": {
        "id": "TlJadNf25gPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is the [link](https://docs.google.com/document/d/1EupL9XjaCt2Hdb7QllS5jkstYJnHv-o_zMagu8EbwbQ/edit?usp=sharing) to the documentation of the project"
      ],
      "metadata": {
        "id": "cBaCfwlu5kAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i) Specifying the Question"
      ],
      "metadata": {
        "id": "DAA3MzbyoAS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use neural networks to translate English text to a local Kenyan language(Kalenjin)."
      ],
      "metadata": {
        "id": "QgtAD41NoGS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii) Defining the metrics of success"
      ],
      "metadata": {
        "id": "c5_pT03EoGyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a model that can accurately translate English text to Kalenjin with an accuracy score of at least 85%"
      ],
      "metadata": {
        "id": "zFqK4GE1oOAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii) Understanding the context"
      ],
      "metadata": {
        "id": "d34eGBTqoOxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several translation websites that mostly translate between international languages such as English to Swahili. In Kenya, there a professional bodies that offer translation and interpretation services. Hiring these services can be quite expensive especially when trying to communicate an important information such as constitution interpretation to a pre-dominantly native speaking community. Having a web application can greatly reduce this burden of having to outsource translation services everytime they are needed. \n"
      ],
      "metadata": {
        "id": "1--RTCl7oWD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iv) Recording the Experimental Design"
      ],
      "metadata": {
        "id": "YnBZZo2goaDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Loading the datasets.\n",
        "\n",
        "2. Cleaning the datasets.\n",
        "\n",
        "3. Preprocessing.\n",
        "\n",
        "4. Creating a TensorFlow model.\n",
        "\n",
        "5. Test Processing.\n",
        "\n",
        "6. Training the model.\n",
        "\n",
        "7. Translating.\n",
        "\n",
        "8. Visualizing the process."
      ],
      "metadata": {
        "id": "u9tk83svoium"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### v) Relevance of the data"
      ],
      "metadata": {
        "id": "KUWkCsvEojSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data used in this project is for performing Text Translation using Neural Networks. The datasets can be found [here](https://drive.google.com/drive/folders/1qJgQvNd99E_U6oitRIToOdXPbjqEHqnG)."
      ],
      "metadata": {
        "id": "CQ6TBbY3tKNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Installations"
      ],
      "metadata": {
        "id": "gfHEgJdgwdVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow-text==2.8.*\""
      ],
      "metadata": {
        "id": "DUQSvTzIwcoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9445719d-09f2-4f45-c9aa-1efa8f550b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text==2.8.*\n",
            "  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (57.4.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.5.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.25.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 65.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (14.0.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.46.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow-text\n",
            "Successfully installed tensorflow-text-2.8.2 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Importing the libraries"
      ],
      "metadata": {
        "id": "nV1hwDh9s1ZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7PCjt709-91",
        "outputId": "d4884b93-35cf-4b1f-e5ea-8d57fb171c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "print(tf.__version__)#to check the tensorflow version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Loading the datasets"
      ],
      "metadata": {
        "id": "_7oYNhBitA8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the datasets\n",
        "english = pd.read_csv('/content/english.txt', sep='delimiter', engine = 'python', header=None)\n",
        "\n",
        "kale = pd.read_csv('/content/kale.txt', sep='delimiter',  engine = 'python', header=None)\n"
      ],
      "metadata": {
        "id": "C6rq-9h7-LVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Previewing the datasets"
      ],
      "metadata": {
        "id": "OXmwhiAZtO7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the shape of the various datasets\n",
        "files = [english, kale]\n",
        "dataset_names = ['English','Kalenjin']\n",
        "for file in files:\n",
        "  #for index in range(len(dataset_names)):\n",
        "    rows, columns = file.shape\n",
        "    print(f'The dataset has {rows} rows and {columns} columns')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6LnmJjnt3PE",
        "outputId": "c433b3cf-4da6-4c7f-a465-4e545e5e4f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset has 176 rows and 1 columns\n",
            "The dataset has 176 rows and 1 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Pre_processing"
      ],
      "metadata": {
        "id": "ItSP4diOv_-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing steps includes\n",
        "\n",
        "- Converting the unicode file to ascii\n",
        "- Creating a space between a word and the punctuation following it\n",
        "eg: “he is a boy.” => “he is a boy .” Reference\n",
        "- Replacing everything with space except (a-z, A-Z, “.”, “?”, “!”, “,”)\n",
        "- Adding a start and an end token to the sentence so that the model know when to start and stop predicting.\n",
        "- Removing the accents\n",
        "- Cleaning the sentences\n",
        "- Return word pairs in the format: [ENGLISH, KALENJIN]\n",
        "- Creating a word -> index mapping (e.g,. 'Further' -> 5) and vice-versa. (e.g., 5 -> 'Further' ) for each language."
      ],
      "metadata": {
        "id": "wKESEW4qxFGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an index column for Kalenjin file\n",
        "kale['index_col'] = kale.index"
      ],
      "metadata": {
        "id": "9Ns6OegFPV-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an index column for Kalenjin file\n",
        "english['index_col'] = kale.index"
      ],
      "metadata": {
        "id": "-LYMqkFVw8Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining the English and Kalenjin file with the Index column\n",
        "df_kale = pd.merge(english, kale, on = 'index_col')"
      ],
      "metadata": {
        "id": "vvDFhp9CObTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the Kalenjin Columns\n",
        "df_kale.head()\n",
        "df_kale.columns = ['feature', 'index', 'target']"
      ],
      "metadata": {
        "id": "LnP8rk38PtsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the Index column in the Kalenjjin file\n",
        "df_kale.columns\n",
        "df_kale = df_kale.drop(columns = ['index'])"
      ],
      "metadata": {
        "id": "UhVBZIzUQow5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first rows on the Kalenjin file\n",
        "df_kale.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CIEp-CbeQypJ",
        "outputId": "8e9dd4ea-1147-49b4-ffdd-198ca4f82b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             feature  \\\n",
              "0  Blessed are the undefiled in the way, who walk...   \n",
              "1  2 Blessed are they that keep his testimonies, ...   \n",
              "2  3 They also do no iniquity: they walk in his w...   \n",
              "3  4 Thou hast commanded us to keep thy precepts ...   \n",
              "4  5 O that my ways were directed to keep thy sta...   \n",
              "\n",
              "                                              target  \n",
              "0  Boiboen che igesunotgei eng’ oret, Che bendote...  \n",
              "1  Boiboen ichek che ribei baornatosiekyik, Che c...  \n",
              "2  Ee, mayaei ichek che ma bo iman; Bendote ortin...  \n",
              "3  Kiing’at konetisiosieguk, Ile kisub eng’ kagii...  \n",
              "4  Ee, kata mie nda ka kimen ortinwekyuk Si kobii...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c18ad65d-6897-4e3e-b579-d823d0064740\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blessed are the undefiled in the way, who walk...</td>\n",
              "      <td>Boiboen che igesunotgei eng’ oret, Che bendote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2 Blessed are they that keep his testimonies, ...</td>\n",
              "      <td>Boiboen ichek che ribei baornatosiekyik, Che c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3 They also do no iniquity: they walk in his w...</td>\n",
              "      <td>Ee, mayaei ichek che ma bo iman; Bendote ortin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4 Thou hast commanded us to keep thy precepts ...</td>\n",
              "      <td>Kiing’at konetisiosieguk, Ile kisub eng’ kagii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5 O that my ways were directed to keep thy sta...</td>\n",
              "      <td>Ee, kata mie nda ka kimen ortinwekyuk Si kobii...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c18ad65d-6897-4e3e-b579-d823d0064740')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c18ad65d-6897-4e3e-b579-d823d0064740 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c18ad65d-6897-4e3e-b579-d823d0064740');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the numbers at the beginning of the feature column\n",
        "df_kale['feature'] = df_kale['feature'].str.replace('\\d+', '')\n",
        "\n",
        "df_kale.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "pi7fEmJ4RKrg",
        "outputId": "74eca7bf-6b93-49be-9f38-9c63c76d2186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             feature  \\\n",
              "0  Blessed are the undefiled in the way, who walk...   \n",
              "1   Blessed are they that keep his testimonies, a...   \n",
              "2   They also do no iniquity: they walk in his ways.   \n",
              "3   Thou hast commanded us to keep thy precepts d...   \n",
              "4   O that my ways were directed to keep thy stat...   \n",
              "\n",
              "                                              target  \n",
              "0  Boiboen che igesunotgei eng’ oret, Che bendote...  \n",
              "1  Boiboen ichek che ribei baornatosiekyik, Che c...  \n",
              "2  Ee, mayaei ichek che ma bo iman; Bendote ortin...  \n",
              "3  Kiing’at konetisiosieguk, Ile kisub eng’ kagii...  \n",
              "4  Ee, kata mie nda ka kimen ortinwekyuk Si kobii...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5cd89548-c2f7-4d3a-904a-a8ecc82a0d2d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blessed are the undefiled in the way, who walk...</td>\n",
              "      <td>Boiboen che igesunotgei eng’ oret, Che bendote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Blessed are they that keep his testimonies, a...</td>\n",
              "      <td>Boiboen ichek che ribei baornatosiekyik, Che c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They also do no iniquity: they walk in his ways.</td>\n",
              "      <td>Ee, mayaei ichek che ma bo iman; Bendote ortin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thou hast commanded us to keep thy precepts d...</td>\n",
              "      <td>Kiing’at konetisiosieguk, Ile kisub eng’ kagii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>O that my ways were directed to keep thy stat...</td>\n",
              "      <td>Ee, kata mie nda ka kimen ortinwekyuk Si kobii...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5cd89548-c2f7-4d3a-904a-a8ecc82a0d2d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5cd89548-c2f7-4d3a-904a-a8ecc82a0d2d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5cd89548-c2f7-4d3a-904a-a8ecc82a0d2d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_kale['feature'] = df_kale['feature'].str.replace('\\d+', '')\n",
        "\n",
        "df_kale['target'] = df_kale['target'].str.replace('\\d+', '')\n",
        "\n",
        "df_kale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "JxzDPMwJc3yR",
        "outputId": "af596986-b01e-409e-a691-94b29e3a577e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               feature  \\\n",
              "0    Blessed are the undefiled in the way, who walk...   \n",
              "1     Blessed are they that keep his testimonies, a...   \n",
              "2     They also do no iniquity: they walk in his ways.   \n",
              "3     Thou hast commanded us to keep thy precepts d...   \n",
              "4     O that my ways were directed to keep thy stat...   \n",
              "..                                                 ...   \n",
              "171   My tongue shall speak of thy word: for all th...   \n",
              "172   Let thine hand help me; for I have chosen thy...   \n",
              "173   I have longed for thy salvation, O Lord; and ...   \n",
              "174   Let my soul live, and it shall praise thee; a...   \n",
              "175   I have gone astray like a lost sheep; seek th...   \n",
              "\n",
              "                                                target  \n",
              "0    Boiboen che igesunotgei eng’ oret, Che bendote...  \n",
              "1    Boiboen ichek che ribei baornatosiekyik, Che c...  \n",
              "2    Ee, mayaei ichek che ma bo iman; Bendote ortin...  \n",
              "3    Kiing’at konetisiosieguk, Ile kisub eng’ kagii...  \n",
              "4    Ee, kata mie nda ka kimen ortinwekyuk Si kobii...  \n",
              "..                                                 ...  \n",
              "171  Ingotien ng’elyeptanyu agobo ng’olyondeng’ung’...  \n",
              "172  Ingochobok eung’ung’ kotoreta; Amu kialewen ko...  \n",
              "173  Kigoama emosto agobo yetuneng’ung’, ee Jehovah...  \n",
              "174  Ingosob sobondanyu, si kolosun; Ak ingotoreta ...  \n",
              "175  Kiabetote ko u kechiriet ne betot; cheng’ kibo...  \n",
              "\n",
              "[176 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-134200fa-2840-4b09-8b70-4d66ba8f90d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blessed are the undefiled in the way, who walk...</td>\n",
              "      <td>Boiboen che igesunotgei eng’ oret, Che bendote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Blessed are they that keep his testimonies, a...</td>\n",
              "      <td>Boiboen ichek che ribei baornatosiekyik, Che c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They also do no iniquity: they walk in his ways.</td>\n",
              "      <td>Ee, mayaei ichek che ma bo iman; Bendote ortin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thou hast commanded us to keep thy precepts d...</td>\n",
              "      <td>Kiing’at konetisiosieguk, Ile kisub eng’ kagii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>O that my ways were directed to keep thy stat...</td>\n",
              "      <td>Ee, kata mie nda ka kimen ortinwekyuk Si kobii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>My tongue shall speak of thy word: for all th...</td>\n",
              "      <td>Ingotien ng’elyeptanyu agobo ng’olyondeng’ung’...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>Let thine hand help me; for I have chosen thy...</td>\n",
              "      <td>Ingochobok eung’ung’ kotoreta; Amu kialewen ko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>I have longed for thy salvation, O Lord; and ...</td>\n",
              "      <td>Kigoama emosto agobo yetuneng’ung’, ee Jehovah...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>Let my soul live, and it shall praise thee; a...</td>\n",
              "      <td>Ingosob sobondanyu, si kolosun; Ak ingotoreta ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>I have gone astray like a lost sheep; seek th...</td>\n",
              "      <td>Kiabetote ko u kechiriet ne betot; cheng’ kibo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>176 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-134200fa-2840-4b09-8b70-4d66ba8f90d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-134200fa-2840-4b09-8b70-4d66ba8f90d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-134200fa-2840-4b09-8b70-4d66ba8f90d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = df_kale['target'].to_list()"
      ],
      "metadata": {
        "id": "npB5UQG4epUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targ = df_kale['feature'].to_list()"
      ],
      "metadata": {
        "id": "5oa6ivQBcE4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Creating tf_dataset"
      ],
      "metadata": {
        "id": "tPz-5B-ve5OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a tf.data.Dataset of strings that shuffles and batches them efficiently:"
      ],
      "metadata": {
        "id": "skpuhugPjU6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tells TensorFlow to create a buffer of at most buffer_size elements, and a background thread to fill that buffer in the background\n",
        "BUFFER_SIZE = len(inp)\n",
        "\n",
        "# Number of samples to be feed into the neural network\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Creating the dataset and shuffling it \n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset "
      ],
      "metadata": {
        "id": "Yn3o0fzUez0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0335de7-32fe-4619-cd5e-22d98d66e13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX98Wa1bHlsE",
        "outputId": "f4976a01-39d0-4306-e81d-d96f7a0e9fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Kigoonata buch bounik; Ago iywei muguleldanyu ng\\xe2\\x80\\x99aleguk.'\n",
            " b'Keera che iywein ak koboiboitu; Amu kiamang\\xe2\\x80\\x99u ng\\xe2\\x80\\x99olyondeng\\xe2\\x80\\x99ung\\xe2\\x80\\x99.'\n",
            " b'Amu kaige sotet ne kiginde iyeto; Ago mautie ng\\xe2\\x80\\x99atutiguk.'\n",
            " b'Amwaitoi agine baornatosieguk eng\\xe2\\x80\\x99 taitab laitorinik, Amalilani.'\n",
            " b'Abaibai eng\\xe2\\x80\\x99 ng\\xe2\\x80\\x99olyondeng\\xe2\\x80\\x99ung\\xe2\\x80\\x99, Ko u ne nyoru tuguk che luulei che chang\\xe2\\x80\\x99.'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b' Princes have persecuted me without a cause: but my heart standeth in awe of thy word.'\n",
            " b' They that fear thee will be glad when they see me; because I have hoped in thy word.'\n",
            " b' For I am become like a bottle in the smoke; yet do I not forget thy statutes.'\n",
            " b' I will speak of thy testimonies also before kings, and will not be ashamed.'\n",
            " b' I rejoice at thy word, as one that findeth great spoil.'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Text processing"
      ],
      "metadata": {
        "id": "_kCgCCkofZql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i) Standardization"
      ],
      "metadata": {
        "id": "QxJjvOzYfeSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the model is dealing with multilingual text with a limited vocabulary standardization of the text is crucial. Steps;\n",
        "1.  Unicode normalization to split accented characters\n",
        "2.  replace compatibility characters with their ASCII equivalents."
      ],
      "metadata": {
        "id": "zbsccb0GlpPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as tf_text"
      ],
      "metadata": {
        "id": "qimZmadJge4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a text normalized and uni encoded\n",
        "sample_text = tf.constant('Kiacheng’in eng’ muguleldanyu tugul')\n",
        "\n",
        "print(sample_text.numpy())\n",
        "print(tf_text.normalize_utf8(sample_text, 'NFKD').numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaXEDlsSfcDu",
        "outputId": "c3eccd67-2844-4755-f038-9296180b38f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Kiacheng\\xe2\\x80\\x99in eng\\xe2\\x80\\x99 muguleldanyu tugul'\n",
            "b'Kiacheng\\xe2\\x80\\x99in eng\\xe2\\x80\\x99 muguleldanyu tugul'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unicode normalization \n",
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "hvbI7A7hgaBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Priniting an example of the original text\n",
        "print(sample_text.numpy().decode())\n",
        "\n",
        "# printing the text afterunicode normalization\n",
        "print(tf_lower_and_split_punct(sample_text).numpy().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTC9ec0vg2w9",
        "outputId": "edf4f6bc-16b5-4ea4-9ea0-b793d6b02ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kiacheng’in eng’ muguleldanyu tugul\n",
            "[START] kiachengin eng muguleldanyu tugul [END]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting and coverting input text to sequences of tokens\n",
        "# max_vocab_size limit RAM usage during the initial scan of the training corpus to discover the vocabulary.\n",
        "max_vocab_size = 25000 \n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)"
      ],
      "metadata": {
        "id": "pcG-ObPshAmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading one epoch of the training data with the adapt method \n",
        "input_text_processor.adapt(inp)\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euDjRG9ZhEPH",
        "outputId": "5faf75d0-f897-4a6f-f486-a7b842bbb725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', ',', 'ak', 'eng', 'amu', 'ngatutiguk']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Kalenjin TextVectorization layer to build the English layer with .adapt() method\n",
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdrMYNIzhKO1",
        "outputId": "ad61eb58-09cf-4abd-e32d-90e293b32bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'thy', '[START]', '[END]', '.', 'i', ',', 'me', 'and']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the layers created to convert a batch of strings into a batch of token IDs\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGz0sQG2hRI9",
        "outputId": "130c17dc-7cc8-460d-929d-4cb07cdd812f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[  2, 331,  62, 170,  23, 389,  28,  42,   4,   3],\n",
              "       [  2, 361,  11,  74,   6, 295,   8,  73,  12,   4],\n",
              "       [  2,   8, 376, 209,  10, 337, 390,  23,  48,   9]])>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the token IDs that are zero-padded that can be turned into a mask\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens)\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "AMFb-oOlhSBZ",
        "outputId": "dde78390-81c9-49bb-9cd4-7b18eec79d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining constants for the model\n",
        "# Embedding layer enables us to convert each word into a fixed length vector of defined size\n",
        "embedding_dim = 512\n",
        "units = 1024"
      ],
      "metadata": {
        "id": "W34FhUHQhXYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  The encoder"
      ],
      "metadata": {
        "id": "yfs8q1paheh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shapechecker"
      ],
      "metadata": {
        "id": "kY6rStF8huvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to prevent loading of data of wrong shape"
      ],
      "metadata": {
        "id": "tTMhMqrChhVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "metadata": {
        "id": "RCF51Lophw-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The BahdanauAttention class handles the weight matrices in a pair of dense layers and calls the builtin implementation\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "CNl8GtARh-_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing to do is build the encoder. The process is as follows:\n",
        "\n",
        "1. Taking a list of token IDs. \n",
        "\n",
        "2. Using the embedding vector for each token.\n",
        "\n",
        "3. Processessing the embeddings into a new sequence "
      ],
      "metadata": {
        "id": "MfGvULMIxdyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the  list of token IDs\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ],
      "metadata": {
        "id": "JF5tj3-LhaoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCGAOYOKhhyU",
        "outputId": "f1ab577a-b4b0-4069-94e8-330e4bc83d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch, shape (batch): (16,)\n",
            "Input batch tokens, shape (batch, s): (16, 16)\n",
            "Encoder output, shape (batch, s, units): (16, 16, 1024)\n",
            "Encoder state, shape (batch, units): (16, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  The attention head"
      ],
      "metadata": {
        "id": "VS6yGwmvh7wE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder uses attention to selectively focus on parts of the input sequence. The attention takes a sequence of vectors as input for each example and returns an \"attention\" vector for each example. "
      ],
      "metadata": {
        "id": "pyENzTlKzQJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention head layer"
      ],
      "metadata": {
        "id": "s8lEay96iFaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a BahdanauAttention layer\n",
        "attention_layer = BahdanauAttention(units)"
      ],
      "metadata": {
        "id": "p3D4q8LqiDDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Excluding the padding\n",
        "(example_tokens != 0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXZOMPBeiLJT",
        "outputId": "059988c0-0b3e-4a26-a80f-e46cf3bb5e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=example_enc_output,\n",
        "    mask=(example_tokens != 0))\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAIY77zQiOLi",
        "outputId": "42027629-76d8-4fd9-89f3-389237686216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch_size, query_seq_length, units):           (16, 2, 1024)\n",
            "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (16, 2, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attention weights across the sequences at t=0\n",
        "# t is used for slicing, for selecting different parts of the data.\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3sxTlpbiSbN",
        "outputId": "c522988f-a4e1-468b-e523-ad7ddbbfa557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the shape of the attention weights\n",
        "attention_weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtKEg3H-iWHO",
        "outputId": "e016c196-f4e1-495f-c589-dbf74b898f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16, 2, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_slice = attention_weights[0, 0].numpy()\n",
        "attention_slice = attention_slice[attention_slice != 0]"
      ],
      "metadata": {
        "id": "nLIfNuypiZs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toogle code"
      ],
      "metadata": {
        "id": "5-EMQsamifCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting attention weights\n",
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "a1 = plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# freeze the xlim\n",
        "plt.xlim(plt.xlim())\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "a2 = plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# zoom in\n",
        "top = max(a1.get_ylim())\n",
        "zoom = 0.85*top\n",
        "a2.set_ylim([0.90*top, top])\n",
        "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC-M1cWiihZl",
        "outputId": "0f2512ff-1cb0-40ab-9663-cc629f3f3a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5dd086b110>]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The decoder"
      ],
      "metadata": {
        "id": "8aJ1-TaViluf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder generates predictions for the next output token.\n",
        "1. The decoder receives the complete encoder output.\n",
        "\n",
        "2. It uses an RNN to keep track of what it has generated so far.\n",
        "\n",
        "3. It uses its RNN output as the query to the attention over the encoder's output, producing the context vector.\n",
        "\n",
        "4. It combines the RNN output and the context vector to generate the attention vector.\n",
        "\n",
        "5. It generates logit predictions for the next token based on the attention vector."
      ],
      "metadata": {
        "id": "QRld9r7zaAao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder class and its initializer creates all the necessary layers.\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    #  Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
      ],
      "metadata": {
        "id": "y2w6cOE_ijbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import typing\n",
        "from typing import Any, Tuple"
      ],
      "metadata": {
        "id": "4BJX5xOzi2Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the call method for this layer which  takes and returns multiple tensors.\n",
        "# Organizing those into simple container classes.\n",
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "metadata": {
        "id": "f7aMUkaXisc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the call method\n",
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(inputs.new_tokens, ('batch', 't'))\n",
        "  shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
        "  shape_checker(inputs.mask, ('batch', 's'))\n",
        "\n",
        "  if state is not None:\n",
        "    shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Lookup the embeddings\n",
        "  vectors = self.embedding(inputs.new_tokens)\n",
        "  shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
        "\n",
        "  # Process one step with the RNN\n",
        "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "  shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  #  Use the RNN output as the query for the attention over the\n",
        "  # encoder output.\n",
        "  context_vector, attention_weights = self.attention(\n",
        "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "  shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "  #  Join the context_vector and rnn_output\n",
        "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "  #  `at = tanh(Wc@[ct; ht])`\n",
        "  attention_vector = self.Wc(context_and_rnn_output)\n",
        "  shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
        "\n",
        "  # Generate logit predictions:\n",
        "  logits = self.fc(attention_vector)\n",
        "  shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
        "\n",
        "  return DecoderOutput(logits, attention_weights), state"
      ],
      "metadata": {
        "id": "hb46LHidjFpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decoder.call = call"
      ],
      "metadata": {
        "id": "zyzKL0jmjnK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing  of the decoder \n",
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ],
      "metadata": {
        "id": "PAUFYYjFjJoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "example_output_tokens = output_text_processor(example_target_batch)\n",
        "\n",
        "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
        "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
      ],
      "metadata": {
        "id": "QOCS-aMZjMex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the decoder\n",
        "dec_result, dec_state = decoder(\n",
        "    inputs = DecoderInput(new_tokens=first_token,\n",
        "                          enc_output=example_enc_output,\n",
        "                          mask=(example_tokens != 0)),\n",
        "    state = example_enc_state\n",
        ")\n",
        "\n",
        "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
        "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8tAW2qgjPX-",
        "outputId": "a3b66268-c306-4fcf-cad1-d16412cd9ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shape: (batch_size, t, output_vocab_size) (16, 1, 486)\n",
            "state shape: (batch_size, dec_units) (16, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling a token with the logits\n",
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
      ],
      "metadata": {
        "id": "r0Sr9yCejyGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding the token as the first word of the output\n",
        "vocab = np.array(output_text_processor.get_vocabulary())\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_36L8TSj3RB",
        "outputId": "72db9b33-a914-4b05-b31f-7dfe5f8b6740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['abideth'],\n",
              "       ['lamp'],\n",
              "       ['remember'],\n",
              "       ['riches'],\n",
              "       ['give']], dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the same enc_output, mask and sampled tokens as new tokens.\n",
        "\n",
        "dec_result, dec_state = decoder(\n",
        "    DecoderInput(sampled_token,\n",
        "                 example_enc_output,\n",
        "                 mask=(example_tokens != 0)),\n",
        "    state=dec_state)"
      ],
      "metadata": {
        "id": "5-K4tG9kkEDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a second set of logits using the decoder\n",
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OSaBZEQkLex",
        "outputId": "52219e51-3240-41a7-ffc7-4cc1627a2b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['deceit'],\n",
              "       ['spoil'],\n",
              "       ['teachers'],\n",
              "       ['despised'],\n",
              "       ['broad']], dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Training"
      ],
      "metadata": {
        "id": "-ng_jdDekShC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model we'll follow the following steps:\n",
        "\n",
        "1. A loss function and optimizer to perform the optimization.\n",
        "\n",
        "2. A training step function defining how to update the model for each input/target batch.\n",
        "\n",
        "3. A training loop to drive the training and save checkpoints."
      ],
      "metadata": {
        "id": "L_FDJCl_gkoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i) Define the loss function"
      ],
      "metadata": {
        "id": "SCjXzI_1k8Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the loss function and optimizer to perform the optimization.\n",
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(y_true, ('batch', 't'))\n",
        "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    shape_checker(loss, ('batch', 't'))\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    shape_checker(mask, ('batch', 't'))\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ],
      "metadata": {
        "id": "C2IcOwyKj8At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii) Implementing the training step"
      ],
      "metadata": {
        "id": "CaII-2g3lLmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing a model class, the training process will be implemented as the train_step method \n",
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "    self.shape_checker = ShapeChecker()\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    self.shape_checker = ShapeChecker()\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ],
      "metadata": {
        "id": "TTsZs60qkcOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting a batch of input_text, target_text from the tf.data.Dataset.\n",
        "def _preprocess(self, input_text, target_text):\n",
        "  self.shape_checker(input_text, ('batch',))\n",
        "  self.shape_checker(target_text, ('batch',))\n",
        "\n",
        "  # Convert the text to token IDs\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  target_tokens = self.output_text_processor(target_text)\n",
        "  self.shape_checker(input_tokens, ('batch', 's'))\n",
        "  self.shape_checker(target_tokens, ('batch', 't'))\n",
        "\n",
        "  # Convert IDs to masks.\n",
        "  input_mask = input_tokens != 0\n",
        "  self.shape_checker(input_mask, ('batch', 's'))\n",
        "\n",
        "  target_mask = target_tokens != 0\n",
        "  self.shape_checker(target_mask, ('batch', 't'))\n",
        "\n",
        "  return input_tokens, input_mask, target_tokens, target_mask"
      ],
      "metadata": {
        "id": "yR_m1uF4kfzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainTranslator._preprocess = _preprocess"
      ],
      "metadata": {
        "id": "s-oqNm4zlZZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the _train_step method\n",
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs  \n",
        "\n",
        "  (input_tokens, input_mask,\n",
        "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "  max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "    self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "    self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                             enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "  # Apply an optimization step\n",
        "  variables = self.trainable_variables \n",
        "  gradients = tape.gradient(average_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Return a dict mapping metric names to current value\n",
        "  return {'batch_loss': average_loss}"
      ],
      "metadata": {
        "id": "fyD9YlGlkhiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainTranslator._train_step = _train_step"
      ],
      "metadata": {
        "id": "j3mVALlPmbLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "  # Run the decoder one step.\n",
        "  decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                               enc_output=enc_output,\n",
        "                               mask=input_mask)\n",
        "\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "  self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
        "  self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "  self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "  # `self.loss` returns the total for non-padded tokens\n",
        "  y = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y, y_pred)\n",
        "\n",
        "  return step_loss, dec_state"
      ],
      "metadata": {
        "id": "dOehrED0md0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainTranslator._loop_step = _loop_step"
      ],
      "metadata": {
        "id": "nNBAPnZwmfFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii) Test the training step"
      ],
      "metadata": {
        "id": "__19iWQPmiwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a TrainTranslator and configuring it for training using the Model.compile method\n",
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "metadata": {
        "id": "kf7vv-CqmnDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the train_step model\n",
        "np.log(output_text_processor.vocabulary_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7YpBQGgmpeq",
        "outputId": "b0443fbc-c788-4fd9-93fb-b5e475b28ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.186208623900494"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the tf.function-wrapped _tf_train_step, to maximize performance while training\n",
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)"
      ],
      "metadata": {
        "id": "65UeaPVZmsKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainTranslator._tf_train_step = _tf_train_step"
      ],
      "metadata": {
        "id": "1pHTE-FLmw4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator.use_tf_function = True"
      ],
      "metadata": {
        "id": "IrDSsPrsmzC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracing the function\n",
        "translator.train_step([example_input_batch, example_target_batch])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_uZLIwbnChM",
        "outputId": "d8acbf41-ba80-4a38-d22f-d0eb5cf4cef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.8452597>}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing out the Batch loss of our model\n",
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNHqOaK0nDRa",
        "outputId": "aaa3ea5c-0871-43d0-b53e-74372ebda22b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.761552>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.62549>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.2934594>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.399683>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.797207>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.543839>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.274994>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.0487204>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.1424274>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.0885406>}\n",
            "\n",
            "CPU times: user 57.6 s, sys: 1.55 s, total: 59.1 s\n",
            "Wall time: 30.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting our batch losses\n",
        "losses = []\n",
        "for n in range(100):\n",
        "  print('.', end='')\n",
        "  logs = translator.train_step([example_input_batch, example_target_batch])\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "print()\n",
        "plt.plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOl01ZMFnOsT",
        "outputId": "c979a08d-20ef-402c-df17-0b18e0b0211a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "....................................................................................................\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5dcbec5dd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building another model to train\n",
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "metadata": {
        "id": "LPHSFGQwnSCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iv) Train the model"
      ],
      "metadata": {
        "id": "pY3YQsfan-O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a couple of epochs by applying the callbacks.Callback method\n",
        "# to collect the history of batch losses\n",
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "metadata": {
        "id": "TNhN4bJHoBSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the batch loss using 37 epochs \n",
        "train_translator.fit(dataset, epochs=37,\n",
        "                     callbacks=[batch_loss])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgA08TIfoEvf",
        "outputId": "5a10db34-1242-4f9b-b042-4495fd3082d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/37\n",
            "11/11 [==============================] - 40s 3s/step - batch_loss: 5.4745\n",
            "Epoch 2/37\n",
            "11/11 [==============================] - 32s 3s/step - batch_loss: 4.4188\n",
            "Epoch 3/37\n",
            "11/11 [==============================] - 33s 3s/step - batch_loss: 3.9716\n",
            "Epoch 4/37\n",
            "11/11 [==============================] - 33s 3s/step - batch_loss: 3.6322\n",
            "Epoch 5/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 3.2632\n",
            "Epoch 6/37\n",
            "11/11 [==============================] - 32s 3s/step - batch_loss: 2.9103\n",
            "Epoch 7/37\n",
            "11/11 [==============================] - 30s 3s/step - batch_loss: 2.5873\n",
            "Epoch 8/37\n",
            "11/11 [==============================] - 32s 3s/step - batch_loss: 2.2833\n",
            "Epoch 9/37\n",
            "11/11 [==============================] - 31s 3s/step - batch_loss: 2.0094\n",
            "Epoch 10/37\n",
            "11/11 [==============================] - 32s 3s/step - batch_loss: 1.7441\n",
            "Epoch 11/37\n",
            "11/11 [==============================] - 31s 3s/step - batch_loss: 1.4873\n",
            "Epoch 12/37\n",
            "11/11 [==============================] - 32s 3s/step - batch_loss: 1.2242\n",
            "Epoch 13/37\n",
            "11/11 [==============================] - 31s 3s/step - batch_loss: 0.9997\n",
            "Epoch 14/37\n",
            "11/11 [==============================] - 30s 3s/step - batch_loss: 0.7815\n",
            "Epoch 15/37\n",
            "11/11 [==============================] - 30s 3s/step - batch_loss: 0.6003\n",
            "Epoch 16/37\n",
            "11/11 [==============================] - 31s 3s/step - batch_loss: 0.4534\n",
            "Epoch 17/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.3484\n",
            "Epoch 18/37\n",
            "11/11 [==============================] - 32s 3s/step - batch_loss: 0.2473\n",
            "Epoch 19/37\n",
            "11/11 [==============================] - 31s 3s/step - batch_loss: 0.1793\n",
            "Epoch 20/37\n",
            "11/11 [==============================] - 32s 3s/step - batch_loss: 0.1258\n",
            "Epoch 21/37\n",
            "11/11 [==============================] - 32s 3s/step - batch_loss: 0.0898\n",
            "Epoch 22/37\n",
            "11/11 [==============================] - 32s 3s/step - batch_loss: 0.0645\n",
            "Epoch 23/37\n",
            "11/11 [==============================] - 35s 3s/step - batch_loss: 0.0485\n",
            "Epoch 24/37\n",
            "11/11 [==============================] - 34s 3s/step - batch_loss: 0.0344\n",
            "Epoch 25/37\n",
            "11/11 [==============================] - 35s 3s/step - batch_loss: 0.0249\n",
            "Epoch 26/37\n",
            "11/11 [==============================] - 35s 3s/step - batch_loss: 0.0202\n",
            "Epoch 27/37\n",
            "11/11 [==============================] - 37s 3s/step - batch_loss: 0.0168\n",
            "Epoch 28/37\n",
            "11/11 [==============================] - 36s 3s/step - batch_loss: 0.0143\n",
            "Epoch 29/37\n",
            "11/11 [==============================] - 39s 4s/step - batch_loss: 0.0126\n",
            "Epoch 30/37\n",
            "11/11 [==============================] - 37s 3s/step - batch_loss: 0.0116\n",
            "Epoch 31/37\n",
            " 9/11 [=======================>......] - ETA: 6s - batch_loss: 0.0107 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the epochs\n",
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ],
      "metadata": {
        "id": "cPsGUSJkqWi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Translate"
      ],
      "metadata": {
        "id": "079uq-8ZqcCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing the full text => texttranslation\n",
        "# This is by inverting the text => token IDsmapping provided by the output_text_processor\n",
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))"
      ],
      "metadata": {
        "id": "pee0S4sxqbZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ],
      "metadata": {
        "id": "Q1zOGt-eqjgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i) Convert IDs to text"
      ],
      "metadata": {
        "id": "pDZbxQB3qp3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the tokens_to_text which converts from token IDs to human readable text.\n",
        "def tokens_to_text(self, result_tokens):\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(result_tokens, ('batch', 't'))\n",
        "  result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "  shape_checker(result_text_tokens, ('batch', 't'))\n",
        "\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                       axis=1, separator=' ')\n",
        "  shape_checker(result_text, ('batch'))\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  shape_checker(result_text, ('batch',))\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "KphbGFO7q9Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Translator.tokens_to_text = tokens_to_text"
      ],
      "metadata": {
        "id": "NCmFMywfqsO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputting some random token IDs and see what it generates (example)\n",
        "example_output_tokens = tf.random.uniform(\n",
        "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
        "    maxval=output_text_processor.vocabulary_size())\n",
        "translator.tokens_to_text(example_output_tokens).numpy()"
      ],
      "metadata": {
        "id": "Po_ThN89rEWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii) Sample from the decoder's predictions"
      ],
      "metadata": {
        "id": "4muFfacgrL9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the decoder's logit outputs and samples token IDs from the distribution\n",
        "def sample(self, logits, temperature):\n",
        "  shape_checker = ShapeChecker()\n",
        "  # 't' is usually 1 here.\n",
        "  shape_checker(logits, ('batch', 't', 'vocab'))\n",
        "  shape_checker(self.token_mask, ('vocab',))\n",
        "\n",
        "  token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "  shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
        "\n",
        "  # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "  logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    new_tokens = tf.argmax(logits, axis=-1)\n",
        "  else: \n",
        "    logits = tf.squeeze(logits, axis=1)\n",
        "    new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                        num_samples=1)\n",
        "\n",
        "  shape_checker(new_tokens, ('batch', 't'))\n",
        "\n",
        "  return new_tokens"
      ],
      "metadata": {
        "id": "aYIHYKfJrKOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Translator.sample = sample"
      ],
      "metadata": {
        "id": "VteBiIP0rS8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random inputs (example)\n",
        "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
        "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
        "example_output_tokens"
      ],
      "metadata": {
        "id": "jNDKC8OyrZ2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii) Implement translation loop"
      ],
      "metadata": {
        "id": "j5Nr3PkorXYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the results into python lists before joining them  using tf.concat into tensors.\n",
        "# This unfolds the graph out to max_length iterations.\n",
        "def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask=(input_tokens!=0))\n",
        "\n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "    attention.append(dec_result.attention_weights)\n",
        "\n",
        "    new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens.append(new_tokens)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = tf.concat(attention, axis=1)\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}"
      ],
      "metadata": {
        "id": "spIWGzjNrVsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Translator.translate = translate_unrolled"
      ],
      "metadata": {
        "id": "uSzYfqdFripi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running a simple input to view the translation\n",
        "%%time\n",
        "input_text = tf.constant([\n",
        "    'kiamwaite eng kutinnyu kiruogutik tugul che bunu kutingung', # \"with my lips have i declared all the judgments of thy mouth\n",
        "    'a kiprutoiyo eng ngony ameungena ngatutiguk', # \"i am a stranger in the earth hide not thy commandments from me\"\n",
        "])\n",
        "\n",
        "\n",
        "result = translator.translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "id": "Gncwm1c6rlmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FrzMLbawJ9pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAKa6BuDe5Or"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7K_qJA-e77L"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "\n",
        "import math\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "try:\n",
        "  nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "  nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_4WkMSrgHci"
      },
      "outputs": [],
      "source": [
        "ref_a = str('all the judgments of thy mouth').split()\n",
        "hyp = str('i have more understanding than all my teachers for thy testimonies are my meditation').split()\n",
        "ref_b = str('all the judgements of your mouth').split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvcLFg5BmUos"
      },
      "outputs": [],
      "source": [
        "print(hyp)\n",
        "print(ref_a)\n",
        "print(ref_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN0cZUkVlQqp"
      },
      "source": [
        "### Sentence BLEU Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uK10FEyrkph"
      },
      "source": [
        "NLTK provides the sentence_bleu() function for evaluating a candidate sentence against one or more reference sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpuL5g2fgLv7"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = [['all', 'the', 'judgments', 'of', 'thy', 'mouth'], ['all', 'the', 'judgements', 'of', 'your', 'mouth']]\n",
        "candidate = ['i', 'have', 'more', 'understanding', 'than', 'all', 'my', 'teachers', 'for', 'thy', 'testimonies', 'are', 'my', 'meditation']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A score of 0.6145 is obtained. This implies the sentences are relatively comparable. 1 implies the two sentences are completly similar and 0 implies the sentences are completely disimilar. \n"
      ],
      "metadata": {
        "id": "Y9lDhuW1KxlM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98F2vG2UlYVQ"
      },
      "source": [
        "### Corpus BLEU Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNA3dO5NrjLw"
      },
      "source": [
        "NLTK also provides a function called corpus_bleu() for calculating the BLEU score for multiple sentences such as a paragraph or a document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fGBeOnzgQgU"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "reference = [[['all', 'the', 'judgments', 'of', 'thy', 'mouth'], ['all', 'the', 'judgements', 'of', 'your', 'mouth']]]\n",
        "candidate = [['i', 'have', 'more', 'understanding', 'than', 'all', 'my', 'teachers', 'for', 'thy', 'testimonies', 'are', 'my', 'meditation']]\n",
        "score = corpus_bleu(reference, candidate)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A corpus BLEU score of 0.6145 is obtained similar to sentence BLEU score. This implies that the translation maintaned its context.A score of 0.6145 indicates that the Quality of the translation is often better than human.\n"
      ],
      "metadata": {
        "id": "myPS3RxpLTvR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu7_PijHlbCU"
      },
      "source": [
        "### Individual N-Gram Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XLc2423rHYu"
      },
      "source": [
        "An individual N-gram score is the evaluation of just matching grams of a specific order, such as single words (1-gram) or word pairs (2-gram or bigram). The weights are specified as a tuple where each index refers to the gram order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emAFpzBRoGcK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1-gram individual BLEU\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = [['all', 'the', 'judgments', 'of', 'thy', 'mouth']]\n",
        "candidate = ['i', 'have', 'more', 'understanding', 'than', 'all', 'my', 'teachers', 'for', 'thy', 'testimonies', 'are', 'my', 'meditation']\n",
        "score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0))\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very low n-gram score was very poor. The original text had six words but the the translation had fourteen letters. This shows the model isn't fully optimized and is still bunching together alot of similar words and needs to be optimized further. "
      ],
      "metadata": {
        "id": "_5M7TMzhMWyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Model"
      ],
      "metadata": {
        "id": "A2mW0SjSs5Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "k4zv5GXft2HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SavedModel format is what we will use to save our model"
      ],
      "metadata": {
        "id": "Y3sVpT2Rs81S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.saved_model.save(translator, 'translator')"
      ],
      "metadata": {
        "id": "sr5wzjeitGNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded = tf.saved_model.load('translator')\n",
        "result = translator.translate(input_text)"
      ],
      "metadata": {
        "id": "9N4_UY5HyYE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = translator.translate(input_text)\n",
        "\n",
        "for tr in result['text']:\n",
        "  print(tr.numpy().decode())\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "QHjos88GyeUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Conclusion"
      ],
      "metadata": {
        "id": "0tcB73funwpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the dataset and metrics we had the translations maintained their context although were too dismilar in terms of structure to the target sentence. \n",
        "From the model metrics, there is a need for the the model to be optimized further. We shall continue building our dataset and optimizing it further. The trained model will be deployed via streamlite  "
      ],
      "metadata": {
        "id": "VlnaPCPMKcGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a). Did we have the right data? The dataset is insufficient, to accurately train the model. A larger dataset with more characters needs in order to improve prediction accuracy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rj2G7roFn1hN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b). Do we need other data to answer our question? Yes"
      ],
      "metadata": {
        "id": "MjuvLmo6bF01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Did we have the right question? Yes"
      ],
      "metadata": {
        "id": "rS0AjWVvbaPM"
      }
    }
  ]
}